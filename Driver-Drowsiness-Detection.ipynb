{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Sound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.mixer.init()\n",
    "sound=pygame.mixer.Sound(r\"beep-09 2.wav\")\n",
    "def play_sound():\n",
    "    if not pygame.mixer.get_busy():\n",
    "        sound.play(-1)\n",
    "        \n",
    "\n",
    "def stop_sound():\n",
    "    \n",
    "    if pygame.mixer.get_busy():\n",
    "        sound.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add function for Progress bar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_progress_bar(frame, ear_value):\n",
    "    bar_width = 250\n",
    "    bar_height = 20\n",
    "    bar_x = 10\n",
    "    bar_y = 120\n",
    "     \n",
    "    min_ear = 0.23\n",
    "    max_ear = 0.30\n",
    "    percentage = (ear_value - min_ear) / (max_ear - min_ear)\n",
    "    percentage = max(0, min(1, percentage))  \n",
    "    \n",
    "    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), (255, 255, 255), 2)\n",
    "    \n",
    "    filled_width = int(bar_width * percentage)\n",
    "    \n",
    "    #if percentage < EAR_THRESHOLD:\n",
    "    if percentage<0.4:\n",
    "        bar_color = (0, 0, 255) \n",
    "    else:\n",
    "        bar_color = (0, 255, 0)  \n",
    "    \n",
    "    cv2.rectangle(frame, (bar_x, bar_y), (bar_x + filled_width, bar_y + bar_height), bar_color, -1)\n",
    "    \n",
    "    #EAR\n",
    "    status_text = f\"Eye Opening: {int(percentage * 100)}% (EAR: {ear_value:.2f})\"\n",
    "    cv2.putText(frame, status_text, (bar_x, bar_y - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do that:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def calculate_ear(eye_points):\n",
    "    \n",
    "    v1 = np.linalg.norm(eye_points[1] - eye_points[5])\n",
    "    v2 = np.linalg.norm(eye_points[2] - eye_points[4])\n",
    "    h = np.linalg.norm(eye_points[0] - eye_points[3])\n",
    "    #  EAR\n",
    "    ear = (v1 + v2) / (2.0 * h)\n",
    "    return ear     # Eye Aspect Ratio\n",
    "\n",
    "\n",
    "with mp_face_mesh.FaceMesh(max_num_faces=1,min_detection_confidence=0.5,\n",
    "                           min_tracking_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    EAR_THRESHOLD = 0.28\n",
    "    EAR_FRAMES =40\n",
    "    blink_count=0\n",
    "    eye_closed=False\n",
    "    start_time=None\n",
    "    \n",
    "    counter=0  \n",
    "    video=cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        H,W,_=frame.shape\n",
    "        mp_face_detection=mp.solutions.face_detection\n",
    "        with mp_face_detection.FaceDetection(min_detection_confidence=0.5\n",
    "                                  ,model_selection=0) as face_detection:\n",
    "            rgb_img=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "            out=face_detection.process(rgb_img)\n",
    "            #print(out.detections)\n",
    "            if out.detections is not None:\n",
    "                for detection in out.detections:\n",
    "                    location_data=detection.location_data\n",
    "                    bbox=location_data.relative_bounding_box\n",
    "                    x1,y1,w,h=bbox.xmin,bbox.ymin,bbox.width,bbox.height\n",
    "\n",
    "                    x1=int(x1*W)\n",
    "                    y1=int(y1*H)\n",
    "                    w=int(w*W)\n",
    "                    h=int(h*H)\n",
    "                    cv2.rectangle(frame,(x1,y1),(x1+w,y1+h),(139,0,0),2)\n",
    "                    print(f'Face_bbox(x1:{x1},y1:{y1},w:{w},h:{h})')\n",
    "                    #cv2.putText(frame,'Face',(x1,y1),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "            \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        results = face_mesh.process(rgb_frame)\n",
    "        \n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                #print(results.multi_face_landmarks)\n",
    "\n",
    "                left_eye = np.array([\n",
    "                    [int(face_landmarks.landmark[33].x * frame.shape[1]), int(face_landmarks.landmark[33].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[160].x * frame.shape[1]), int(face_landmarks.landmark[160].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[158].x * frame.shape[1]), int(face_landmarks.landmark[158].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[133].x * frame.shape[1]), int(face_landmarks.landmark[133].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[153].x * frame.shape[1]), int(face_landmarks.landmark[153].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[144].x * frame.shape[1]), int(face_landmarks.landmark[144].y * frame.shape[0])]\n",
    "                ])\n",
    "                \n",
    "                right_eye = np.array([\n",
    "                    [int(face_landmarks.landmark[362].x * frame.shape[1]), int(face_landmarks.landmark[362].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[387].x * frame.shape[1]), int(face_landmarks.landmark[387].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[386].x * frame.shape[1]), int(face_landmarks.landmark[386].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[263].x * frame.shape[1]), int(face_landmarks.landmark[263].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[373].x * frame.shape[1]), int(face_landmarks.landmark[373].y * frame.shape[0])],\n",
    "                    [int(face_landmarks.landmark[380].x * frame.shape[1]), int(face_landmarks.landmark[380].y * frame.shape[0])]\n",
    "                ])\n",
    "        \n",
    "                left_ear = calculate_ear(left_eye)\n",
    "                right_ear = calculate_ear(right_eye)\n",
    "                \n",
    "                ear = (left_ear + right_ear) / 2.0\n",
    "                draw_progress_bar(frame, ear)\n",
    "                if ear>EAR_THRESHOLD:\n",
    "                    eye_closed=True\n",
    "                    \n",
    "                else:\n",
    "                    if eye_closed:\n",
    "                        blink_count+=1\n",
    "                    eye_closed=False\n",
    "                cv2.putText(frame, f\"Blinks:{blink_count}\", (10, 180), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 1, (139, 0,0), 2)\n",
    "\n",
    "\n",
    "                if ear < EAR_THRESHOLD:\n",
    "                    cv2.putText(frame, \"Eyes Closed\", (10, 30), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    if start_time is None:\n",
    "                        start_time=time.time() \n",
    "                    counter +=1\n",
    "                    if counter >= EAR_FRAMES:\n",
    "                        elapsed_time=time.time()-start_time  \n",
    "                        cv2.putText(frame, f\"Eyes Closed for {elapsed_time:.1f} sec\", (10, 30), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                        \n",
    "                        threading.Thread(target=play_sound).start()\n",
    "                        \n",
    "                else:\n",
    "                    counter=0\n",
    "                    cv2.putText(frame, \"Eyes Open\", (10, 30), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                    stop_sound()\n",
    "                    start_time=None\n",
    "                \n",
    "                cv2.putText(frame, f\"EAR: {ear:.2f}\", (10, 70), \n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 1, (139,0,0), 2)\n",
    "                \n",
    "                \n",
    "                for point in left_eye:\n",
    "                    cv2.circle(frame, tuple(point), 2, (0, 255, 0), -1)\n",
    "                for point in right_eye:\n",
    "                    cv2.circle(frame, tuple(point), 2, (0, 255, 0), -1)\n",
    "                \n",
    "                for i in range(3):\n",
    "                    cv2.line(frame, tuple(left_eye[i]), tuple(left_eye[i+3]), (0, 255, 0), 1)\n",
    "                    cv2.line(frame, tuple(right_eye[i]), tuple(right_eye[i+3]), (0, 255, 0), 1)\n",
    "        \n",
    "        cv2.imshow('Eye State Detection', frame)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "git",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
